{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-18T06:17:53.089623Z",
     "start_time": "2023-11-18T06:17:52.139144Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Fetch Taxi Data\n",
    "\n",
    "# Dataset\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet --no-check-certificate\n",
    "\n",
    "# Data dictionary\n",
    "!wget https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf --no-check-certificate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eb7dec5c6c1394f"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_table() got an unexpected keyword argument 'chunksize'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 14\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## Parquet Reading\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# parquet_file = pq.ParquetFile('yellow_tripdata_2023-01.parquet')\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# dfp.head()\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m dfp \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43myellow_tripdata_2023-01.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/psql-pipeline/lib/python3.11/site-packages/pandas/io/parquet.py:670\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001B[0m\n\u001B[1;32m    667\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    668\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[0;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_backend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_backend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/psql-pipeline/lib/python3.11/site-packages/pandas/io/parquet.py:272\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001B[0m\n\u001B[1;32m    265\u001B[0m path_or_handle, handles, filesystem \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[1;32m    266\u001B[0m     path,\n\u001B[1;32m    267\u001B[0m     filesystem,\n\u001B[1;32m    268\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    269\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    270\u001B[0m )\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 272\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_handle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m     result \u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mto_pandas(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_pandas_kwargs)\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mTypeError\u001B[0m: read_table() got an unexpected keyword argument 'chunksize'"
     ]
    }
   ],
   "source": [
    "## Parquet Reading\n",
    "# parquet_file = pq.ParquetFile('yellow_tripdata_2023-01.parquet')\n",
    "# \n",
    "# iter = parquet_file.iter_batches(batch_size=1)\n",
    "# \n",
    "# # Use a list comprehension to get the first batch\n",
    "# first_batch = next(iter)\n",
    "# \n",
    "# # Convert the batch to a Pandas DataFrame\n",
    "# dfp = first_batch.to_pandas()\n",
    "# \n",
    "# dfp.head()\n",
    "\n",
    "dfp = pd.read_parquet('yellow_tripdata_2023-01.parquet')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T06:34:04.976605Z",
     "start_time": "2023-11-18T06:34:04.880865Z"
    }
   },
   "id": "70636b0f562d69bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Parquet to CSV\n",
    "\n",
    "# Read parquet\n",
    "dfp = pd.read_parquet('yellow_tripdata_2023-01.parquet')\n",
    "\n",
    "## Process problematic columns \n",
    "\n",
    "# df.iloc[:,6].head()\n",
    "# df.drop(df.columns[0], axis=1, inplace=True)\n",
    "# df.iloc[:,6].dtype\n",
    "# df.iloc[:, 6] = df.iloc[:, 6].astype(str)\n",
    "# df.iloc[:, 6].astype(str).dtype\n",
    "# print(df.iloc[:, 6].unique())\n",
    "# condition = df[(df.iloc[:, 6] != \"N\") & (df.iloc[:, 6] != \"Y\")].index\n",
    "\n",
    "# Drop rows that don't meet the condition\n",
    "# df = df.drop(index=condition)\n",
    "\n",
    "# Reset the index if needed\n",
    "# df = df.reset_index(drop=True)\n",
    "# df.iloc[:, 6].value_counts()\n",
    "# dfp.iloc[:, 6].value_counts()\n",
    "\n",
    "\n",
    "## Save as CSV\n",
    "# Index = False to not have 1st column as index\n",
    "# df.to_csv('yellow_tripdata_2023-01.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dc38ad35435802a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('yellow_tripdata_2023-01.csv', nrows=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T21:56:04.766788Z",
     "start_time": "2023-11-17T21:56:04.704323Z"
    }
   },
   "id": "3ca694b6e0b7f4ee"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Cleaning\n",
    "\n",
    "# pickup and drop off datetime should be as \"TIMESTAMP\", not \"TEXT\" in schema\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T21:56:08.062716Z",
     "start_time": "2023-11-17T21:56:08.048415Z"
    }
   },
   "id": "7cb5bcf1c03df01d"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# SQL Alchemy\n",
    "# type of db://user/:password@hostname:port/db_name\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:27:12.478121Z",
     "start_time": "2023-11-17T22:27:12.456509Z"
    }
   },
   "id": "b06d2e8830b86702"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<sqlalchemy.engine.base.Connection at 0x12d30a810>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.connect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:27:13.828060Z",
     "start_time": "2023-11-17T22:27:13.766518Z"
    }
   },
   "id": "727b3d214460e567"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "  schemaname         tablename tableowner tablespace  hasindexes  hasrules  \\\n0     public  yellow_taxi_data       root       None        True     False   \n\n   hastriggers  rowsecurity  \n0        False        False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>schemaname</th>\n      <th>tablename</th>\n      <th>tableowner</th>\n      <th>tablespace</th>\n      <th>hasindexes</th>\n      <th>hasrules</th>\n      <th>hastriggers</th>\n      <th>rowsecurity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>public</td>\n      <td>yellow_taxi_data</td>\n      <td>root</td>\n      <td>None</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test connection with test query\n",
    "dummy_query = \"\"\"\n",
    "SELECT 1 as number;\n",
    "\"\"\"\n",
    "\n",
    "# describe tables query; wont work because its psql specific\n",
    "psqlQuery = \"\"\"\n",
    "\\dt\n",
    "\"\"\"\n",
    "\n",
    "## \\dt as generic SQL\n",
    "query = \"\"\"\n",
    "select * from pg_catalog.pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema';\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, con=engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T23:21:21.638769Z",
     "start_time": "2023-11-17T23:21:21.441951Z"
    }
   },
   "id": "51f4e86889f2663f"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\tairport_fee FLOAT(53)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Show df in DDL (Data Definition Language)\n",
    "# describes how the  data will be shown in SQL\n",
    "# generic SQL statement. may or may not work with Postgres\n",
    "# print(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\n",
    "\n",
    "# definition with Postgres\n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:27:16.966206Z",
     "start_time": "2023-11-17T22:27:16.921855Z"
    }
   },
   "id": "8a57eb9644c3a88c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Chunk the data\n",
    "df_iter = pd.read_csv('yellow_tripdata_2023-01.csv', iterator=True, chunksize=100000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:30:47.461759Z",
     "start_time": "2023-11-17T22:30:47.442789Z"
    }
   },
   "id": "db39643bf33b3e3c"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df = next(df_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:30:52.798393Z",
     "start_time": "2023-11-17T22:30:52.362784Z"
    }
   },
   "id": "632b5734d9d23e5a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Preprocess chunk\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:30:53.693180Z",
     "start_time": "2023-11-17T22:30:53.658923Z"
    }
   },
   "id": "4cf9ef64060fca4b"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are chunking, so insert just the row headers first\n",
    "df.head(0).to_sql(name=\"yellow_taxi_data\",con=engine, if_exists='replace')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:28:08.865068Z",
     "start_time": "2023-11-17T22:28:08.809865Z"
    }
   },
   "id": "22e9dc06b9afa23"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 456 ms, total: 7.48 s\n",
      "Wall time: 14.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert a chunk, and time it\n",
    "%time df.to_sql(name=\"yellow_taxi_data\",con=engine, if_exists='append')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:33:15.783372Z",
     "start_time": "2023-11-17T22:33:01.126579Z"
    }
   },
   "id": "f00c5e72789c171c"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from time import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:38:19.641555Z",
     "start_time": "2023-11-17T22:38:19.634994Z"
    }
   },
   "id": "9523e6a5089c4c01"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk... 15.200 seconds\n",
      "inserted another chunk... 14.556 seconds\n",
      "inserted another chunk... 14.613 seconds\n",
      "inserted another chunk... 14.183 seconds\n",
      "inserted another chunk... 16.016 seconds\n",
      "inserted another chunk... 13.456 seconds\n",
      "inserted another chunk... 12.361 seconds\n",
      "inserted another chunk... 12.969 seconds\n",
      "inserted another chunk... 14.230 seconds\n",
      "inserted another chunk... 13.837 seconds\n",
      "inserted another chunk... 12.968 seconds\n",
      "inserted another chunk... 12.591 seconds\n",
      "inserted another chunk... 14.028 seconds\n",
      "inserted another chunk... 15.712 seconds\n",
      "inserted another chunk... 13.423 seconds\n",
      "inserted another chunk... 12.537 seconds\n",
      "inserted another chunk... 12.572 seconds\n",
      "inserted another chunk... 13.297 seconds\n",
      "inserted another chunk... 13.365 seconds\n",
      "inserted another chunk... 13.104 seconds\n",
      "inserted another chunk... 12.667 seconds\n",
      "inserted another chunk... 12.929 seconds\n",
      "inserted another chunk... 13.609 seconds\n",
      "inserted another chunk... 15.265 seconds\n",
      "inserted another chunk... 13.028 seconds\n",
      "inserted another chunk... 12.726 seconds\n",
      "inserted another chunk... 12.503 seconds\n",
      "inserted another chunk... 12.800 seconds\n",
      "inserted another chunk... 13.746 seconds\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m      4\u001B[0m     t_start \u001B[38;5;241m=\u001B[39m time()\n\u001B[0;32m----> 5\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(df_iter)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Preprocess chunk\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     df\u001B[38;5;241m.\u001B[39mtpep_pickup_datetime \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df\u001B[38;5;241m.\u001B[39mtpep_pickup_datetime)\n",
      "File \u001B[0;32m~/anaconda3/envs/psql-pipeline/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1668\u001B[0m, in \u001B[0;36mTextFileReader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1666\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m   1667\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1668\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1669\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m   1670\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/anaconda3/envs/psql-pipeline/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1777\u001B[0m, in \u001B[0;36mTextFileReader.get_chunk\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m   1775\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[1;32m   1776\u001B[0m     size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnrows \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_currow)\n\u001B[0;32m-> 1777\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/psql-pipeline/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1741\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m     (\n\u001B[1;32m   1745\u001B[0m         index,\n\u001B[1;32m   1746\u001B[0m         columns,\n\u001B[1;32m   1747\u001B[0m         col_dict,\n\u001B[0;32m-> 1748\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1749\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1750\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1751\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1752\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/anaconda3/envs/psql-pipeline/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32mparsers.pyx:868\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## insert all chunks iteratively\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        t_start = time()\n",
    "        df = next(df_iter)\n",
    "        \n",
    "        # Preprocess chunk\n",
    "        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "        \n",
    "        # insert\n",
    "        df.to_sql(name=\"yellow_taxi_data\",con=engine, if_exists='append')\n",
    "        t_end = time()\n",
    "        \n",
    "        print('inserted another chunk... %.3f seconds' % (t_end-t_start))\n",
    "    except StopIteration:\n",
    "        print(\"Finished inserting all chunks.\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T22:45:01.614077Z",
     "start_time": "2023-11-17T22:38:26.909867Z"
    }
   },
   "id": "e232a3a12213a28a"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "   index  VendorID tpep_pickup_datetime tpep_dropoff_datetime  \\\n0      0         2  2023-01-01 00:32:10   2023-01-01 00:40:36   \n1      1         2  2023-01-01 00:55:08   2023-01-01 01:01:27   \n2      2         2  2023-01-01 00:25:04   2023-01-01 00:37:49   \n3      3         1  2023-01-01 00:03:48   2023-01-01 00:13:25   \n4      4         2  2023-01-01 00:10:29   2023-01-01 00:21:19   \n5      5         2  2023-01-01 00:50:34   2023-01-01 01:02:52   \n6      6         2  2023-01-01 00:09:22   2023-01-01 00:19:49   \n7      7         2  2023-01-01 00:27:12   2023-01-01 00:49:56   \n8      8         2  2023-01-01 00:21:44   2023-01-01 00:36:40   \n9      9         2  2023-01-01 00:39:42   2023-01-01 00:50:36   \n\n   passenger_count  trip_distance  RatecodeID store_and_fwd_flag  \\\n0              1.0           0.97         1.0                  N   \n1              1.0           1.10         1.0                  N   \n2              1.0           2.51         1.0                  N   \n3              0.0           1.90         1.0                  N   \n4              1.0           1.43         1.0                  N   \n5              1.0           1.84         1.0                  N   \n6              1.0           1.66         1.0                  N   \n7              1.0          11.70         1.0                  N   \n8              1.0           2.95         1.0                  N   \n9              1.0           3.01         1.0                  N   \n\n   PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  \\\n0           161           141             2          9.3   1.00      0.5   \n1            43           237             1          7.9   1.00      0.5   \n2            48           238             1         14.9   1.00      0.5   \n3           138             7             1         12.1   7.25      0.5   \n4           107            79             1         11.4   1.00      0.5   \n5           161           137             1         12.8   1.00      0.5   \n6           239           143             1         12.1   1.00      0.5   \n7           142           200             1         45.7   1.00      0.5   \n8           164           236             1         17.7   1.00      0.5   \n9           141           107             2         14.9   1.00      0.5   \n\n   tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n0        0.00           0.0                    1.0         14.30   \n1        4.00           0.0                    1.0         16.90   \n2       15.00           0.0                    1.0         34.90   \n3        0.00           0.0                    1.0         20.85   \n4        3.28           0.0                    1.0         19.68   \n5       10.00           0.0                    1.0         27.80   \n6        3.42           0.0                    1.0         20.52   \n7       10.74           3.0                    1.0         64.44   \n8        5.68           0.0                    1.0         28.38   \n9        0.00           0.0                    1.0         19.90   \n\n   congestion_surcharge  airport_fee  \n0                   2.5         0.00  \n1                   2.5         0.00  \n2                   2.5         0.00  \n3                   0.0         1.25  \n4                   2.5         0.00  \n5                   2.5         0.00  \n6                   2.5         0.00  \n7                   2.5         0.00  \n8                   2.5         0.00  \n9                   2.5         0.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>VendorID</th>\n      <th>tpep_pickup_datetime</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>RatecodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>PULocationID</th>\n      <th>DOLocationID</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n      <th>congestion_surcharge</th>\n      <th>airport_fee</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2</td>\n      <td>2023-01-01 00:32:10</td>\n      <td>2023-01-01 00:40:36</td>\n      <td>1.0</td>\n      <td>0.97</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>161</td>\n      <td>141</td>\n      <td>2</td>\n      <td>9.3</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>14.30</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2023-01-01 00:55:08</td>\n      <td>2023-01-01 01:01:27</td>\n      <td>1.0</td>\n      <td>1.10</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>43</td>\n      <td>237</td>\n      <td>1</td>\n      <td>7.9</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>4.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>16.90</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2023-01-01 00:25:04</td>\n      <td>2023-01-01 00:37:49</td>\n      <td>1.0</td>\n      <td>2.51</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>48</td>\n      <td>238</td>\n      <td>1</td>\n      <td>14.9</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>15.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>34.90</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2023-01-01 00:03:48</td>\n      <td>2023-01-01 00:13:25</td>\n      <td>0.0</td>\n      <td>1.90</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>138</td>\n      <td>7</td>\n      <td>1</td>\n      <td>12.1</td>\n      <td>7.25</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>20.85</td>\n      <td>0.0</td>\n      <td>1.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2</td>\n      <td>2023-01-01 00:10:29</td>\n      <td>2023-01-01 00:21:19</td>\n      <td>1.0</td>\n      <td>1.43</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>107</td>\n      <td>79</td>\n      <td>1</td>\n      <td>11.4</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>3.28</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>19.68</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>2</td>\n      <td>2023-01-01 00:50:34</td>\n      <td>2023-01-01 01:02:52</td>\n      <td>1.0</td>\n      <td>1.84</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>161</td>\n      <td>137</td>\n      <td>1</td>\n      <td>12.8</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>10.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>27.80</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>2</td>\n      <td>2023-01-01 00:09:22</td>\n      <td>2023-01-01 00:19:49</td>\n      <td>1.0</td>\n      <td>1.66</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>239</td>\n      <td>143</td>\n      <td>1</td>\n      <td>12.1</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>3.42</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>20.52</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>2</td>\n      <td>2023-01-01 00:27:12</td>\n      <td>2023-01-01 00:49:56</td>\n      <td>1.0</td>\n      <td>11.70</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>142</td>\n      <td>200</td>\n      <td>1</td>\n      <td>45.7</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>10.74</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>64.44</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>2</td>\n      <td>2023-01-01 00:21:44</td>\n      <td>2023-01-01 00:36:40</td>\n      <td>1.0</td>\n      <td>2.95</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>164</td>\n      <td>236</td>\n      <td>1</td>\n      <td>17.7</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>5.68</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>28.38</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>2</td>\n      <td>2023-01-01 00:39:42</td>\n      <td>2023-01-01 00:50:36</td>\n      <td>1.0</td>\n      <td>3.01</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>141</td>\n      <td>107</td>\n      <td>2</td>\n      <td>14.9</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>19.90</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check results with sample query\n",
    "query = \"\"\"\n",
    "select * from yellow_taxi_data LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, con=engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T23:24:22.838324Z",
     "start_time": "2023-11-17T23:24:22.724110Z"
    }
   },
   "id": "c7dee0041673f0dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
